"""WhisperS2Tã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£… (Template Methodç‰ˆ)"""
import os
import logging
import tempfile
import time
import soundfile as sf
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
import numpy as np

from .base_engine import BaseEngine
from .model_memory_cache import ModelMemoryCache
from .library_preloader import LibraryPreloader
from livecap_core.languages import Languages

# ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¹è§£æ±ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¨ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from livecap_core.utils import detect_device, get_temp_dir

logger = logging.getLogger(__name__)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å®šå€¤ï¼ˆã‚¯ãƒ©ã‚¹å¤‰æ•°ï¼‰
CPU_SPEED_ESTIMATES = {
    'base': '3-5x real-time',
    'large-v3': '0.1-0.3x real-time (VERY SLOW)'
}


class WhisperS2TEngine(BaseEngine):
    """WhisperS2TéŸ³å£°èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ (Template Methodç‰ˆ)"""

    def __init__(
        self,
        device: Optional[str] = None,
        # ã‚«ãƒ†ã‚´ãƒªA: ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEngineMetadata.default_params ã§å®šç¾©ï¼‰
        language: str = "ja",
        model_size: str = "base",
        batch_size: int = 24,
        use_vad: bool = True,
        **kwargs,
    ):
        """ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–"""
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’è¨­å®šã—ã¦engine_nameã‚’è¨­å®šï¼ˆBaseEngineåˆæœŸåŒ–å‰ã«å¿…è¦ï¼‰
        self.engine_name = f'whispers2t_{model_size}'
        self.model_size = model_size
        self.language = language
        self.batch_size = batch_size
        self.use_vad = use_vad

        # cuDNNè¨­å®šï¼ˆGPUä½¿ç”¨æ™‚ã®å®‰å®šæ€§å‘ä¸Šï¼‰
        os.environ['CUDNN_DETERMINISTIC'] = '1'
        os.environ['CUDNN_BENCHMARK'] = '0'

        # ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•æ¤œå‡ºã¨è¨­å®šï¼ˆå…±é€šé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
        self.device, self.compute_type = detect_device(device, "WhisperS2T")

        # large-v3ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ã®è­¦å‘Š
        if self.model_size == 'large-v3' and self.device == 'cpu':
            logger.warning("âš ï¸ WhisperS2T Large-v3 on CPU will be VERY SLOW! Consider using GPU or smaller model.")

        # BaseEngineåˆæœŸåŒ–ï¼ˆget_model_metadata()ãŒå‘¼ã°ã‚Œã‚‹ï¼‰
        # detect_deviceã§å–å¾—ã—ãŸæ­£ã—ã„deviceå€¤ã‚’æ¸¡ã™ï¼ˆNoneã§ã¯ãªãï¼‰
        super().__init__(self.device, **kwargs)

        # äº‹å‰ãƒ­ãƒ¼ãƒ‰é–‹å§‹
        LibraryPreloader.start_preloading('whispers2t')

        # å›ºå®šã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        self._tmp_dir = get_temp_dir("whispers2t")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°è¨­å®šï¼ˆkwargs ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ Falseï¼‰
        self._enable_profiling = kwargs.get('profile', False)

        # åˆæœŸåŒ–å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if self.device == 'cuda':
            logger.info(f"âœ… WhisperS2T {model_size} engine initialized (GPU mode: {self.compute_type})")
        else:
            logger.info(f"WhisperS2T {model_size} engine initialized (CPU mode)")
    
    def get_model_metadata(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        descriptions = {
            'base': 'Whisper Base - Good balance',
            'large-v3': 'Whisper Large v3 - Best accuracy'
        }

        return {
            'name': f'openai/whisper-{self.model_size}',
            'version': 'v3' if 'v3' in self.model_size else 'v2',
            'format': 'ct2',
            'language': 'multilingual',
            'description': descriptions.get(self.model_size, descriptions['base']),
            'model_size': self.model_size
        }
    
    def _check_dependencies(self) -> None:
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ (Step 1: 0-10%)"""
        self.report_progress(5, self.get_status_message("checking_availability", engine_name="WhisperS2T"))
        LibraryPreloader.wait_for_preload(timeout=2.0)

        try:
            import whisper_s2t
        except ImportError:
            raise ImportError("WhisperS2T is not installed. Please run: pip install whisper-s2t")

        if self.device == 'cuda':
            try:
                import torch
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = False
                torch.backends.cudnn.deterministic = True
            except ImportError:
                pass

        self.report_progress(10, self.get_status_message("dependencies_complete"))
    
    def _get_local_model_path(self, models_dir: Path) -> Path:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾— (Step 2: 10-15%)"""
        model_path = models_dir / f"whisper-{self.model_size}"
        self.report_progress(15, self.get_status_message("model_info", model_name=f"whisper-{self.model_size}"))
        return model_path

    def _is_model_cached(self, model_path: Path) -> bool:
        """WhisperS2Tã¯å†…éƒ¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ç®¡ç†ã™ã‚‹ãŸã‚ã€å¸¸ã«Trueã‚’è¿”ã™"""
        return True

    def _verify_model_integrity(self, model_path) -> bool:
        """WhisperS2Tã¯ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿä½“ã«ä¾å­˜ã—ãªã„ãŸã‚å¸¸ã«True"""
        return True
    
    def _download_model(self, target_path: Path, progress_callback, model_manager=None) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Step 3: 15-70%)"""
        self.report_progress(70, self.get_status_message("model_ready", engine_name="WhisperS2T", model_name=self.model_size))
    
    def _load_model_from_path(self, model_path: Path) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ (Step 4: 70-90%)"""
        import whisper_s2t

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        cache_key = f"whispers2t_{self.model_size}_{self.device}_{self.compute_type}"
        cached_model = ModelMemoryCache.get(cache_key)

        if cached_model is not None:
            logger.info(f"ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—: {cache_key}")
            self.report_progress(90, self.get_status_message("loading_from_memory_cache"))
            return cached_model

        if self.model_size == 'large-v3' and self.device == 'cpu':
            logger.warning("ğŸ“Š WhisperS2T Large-v3 requires ~10GB system memory on CPU")

        self.report_progress(75, self.get_status_message("initializing_model", engine_name="WhisperS2T", model_name=self.model_size))
        
        try:
            # WhisperS2Tãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            model = whisper_s2t.load_model(
                model_identifier=self.model_size,
                backend='CTranslate2',
                device=self.device,
                compute_type=self.compute_type
            )
            
            self.report_progress(85, self.get_status_message("initialization_success", engine_name="WhisperS2T"))

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            ModelMemoryCache.set(cache_key, model, strong=True)

            if self.device == 'cuda':
                logger.info(f"âœ… WhisperS2T {self.model_size} loaded on GPU")
            elif self.model_size == 'large-v3':
                logger.info(f"ğŸ“Š WhisperS2T {self.model_size} on CPU: {CPU_SPEED_ESTIMATES.get(self.model_size)}")

            self.report_progress(90, self.get_status_message("model_ready_simple", engine_name="WhisperS2T"))
            return model
            
        except Exception as e:
            if "cuDNN" in str(e) and self.device == 'cuda':
                logger.warning(f"cuDNN error detected, falling back to CPU: {e}")
                self.device = 'cpu'
                self.compute_type = 'float32'

                model = whisper_s2t.load_model(
                    model_identifier=self.model_size,
                    backend='CTranslate2',
                    device='cpu',
                    compute_type='float32'
                )

                ModelMemoryCache.set(f"whispers2t_{self.model_size}_cpu_float32", model, strong=True)
                self.report_progress(90, self.get_status_message("model_ready_cpu_mode", engine_name="WhisperS2T"))
                return model
            else:
                logger.error(f"Failed to load WhisperS2T model: {e}")
                raise
    
    def _configure_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«è¨­å®š (Step 5: 90-100%)"""
        if self.model is None:
            raise RuntimeError("Model not loaded")

        self.report_progress(95, self.get_status_message("final_configuration", engine_name="WhisperS2T"))

        logger.info(f"WhisperS2T {self.model_size} initialized")

        self.report_progress(100, self.get_status_message("initialization_complete", engine_name="WhisperS2T"))
    
    def transcribe(self, audio_data: np.ndarray, sample_rate: int) -> Tuple[str, float]:
        """
        éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹

        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpyé…åˆ—ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

        Returns:
            (transcription_text, confidence_score)ã®ã‚¿ãƒ—ãƒ«
        """
        # WhisperS2Tã¯é•·æ™‚é–“éŸ³å£°ã‚‚å‡¦ç†å¯èƒ½
        # ç’°å¢ƒå¤‰æ•°åˆ‡æ›¿ã¯ä¸è¦ï¼ˆå›ºå®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼‰
        return self._transcribe_single_chunk(audio_data, sample_rate)
    
    def _transcribe_single_chunk(self, audio_data: np.ndarray, sample_rate: int) -> Tuple[str, float]:
        """
        å˜ä¸€ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰

        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpyé…åˆ—ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

        Returns:
            (transcription_text, confidence_score)ã®ã‚¿ãƒ—ãƒ«
        """
        if not self._initialized or self.model is None:
            raise RuntimeError("Engine not initialized. Call load_model() first.")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹
        if self._enable_profiling:
            profile_times = {}
            total_start = time.perf_counter()
            
        # 16kHzã«å¤‰æ›
        required_sr = 16000
        if sample_rate != required_sr:
            if self._enable_profiling:
                resample_start = time.perf_counter()

            from scipy.signal import resample_poly
            from math import gcd

            g = gcd(sample_rate, required_sr)
            audio_data = resample_poly(audio_data, required_sr // g, sample_rate // g).astype(np.float32)

            if self._enable_profiling:
                profile_times['resample'] = (time.perf_counter() - resample_start) * 1000
            
        # æ­£è¦åŒ–
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        if np.abs(audio_data).max() > 1.0:
            audio_data = audio_data / np.abs(audio_data).max()
            
        
        # éŸ³å£°ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
        min_samples = int(0.1 * 16000)  # æœ€å°0.1ç§’
        if len(audio_data) < min_samples:
            return "", 1.0
            
        try:
            # å…¥åŠ›è¨€èªã‚’å–å¾—ï¼ˆself.language ã‚’ä½¿ç”¨ï¼‰
            input_language = self.language

            # WhisperS2Tç”¨ã®è¨€èªã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
            # WhisperS2Tã¯'zh-CN'ã‚„'zh-TW'ã‚’å—ã‘ä»˜ã‘ãšã€'zh'ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆ
            lang_info = Languages.get_info(input_language)
            if lang_info:
                # Languages.pyã®asr_codeã‚’ä½¿ç”¨ï¼ˆzh-CN/zh-TW â†’ zh ã¸ã®å¤‰æ›ãŒå®šç¾©æ¸ˆã¿ï¼‰
                whisper_language = lang_info.asr_code
            else:
                # è¨€èªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                whisper_language = input_language

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            if self._enable_profiling:
                io_start = time.perf_counter()

            with tempfile.NamedTemporaryFile(dir=self._tmp_dir, suffix='.wav', delete=False) as tmp_file:
                tmp_path = tmp_file.name
                sf.write(tmp_path, audio_data, 16000)

            if self._enable_profiling:
                profile_times['wav_write'] = (time.perf_counter() - io_start) * 1000

            try:
                if self._enable_profiling:
                    inference_start = time.perf_counter()

                # WhisperS2Tã§æ–‡å­—èµ·ã“ã—
                if self.use_vad:
                    outputs = self.model.transcribe_with_vad(
                        [tmp_path],
                        lang_codes=[whisper_language],
                        tasks=["transcribe"],
                        initial_prompts=[None],
                        batch_size=self.batch_size
                    )
                else:
                    outputs = self.model.transcribe(
                        [tmp_path],
                        lang_codes=[whisper_language],
                        tasks=["transcribe"],
                        initial_prompts=[None],
                        batch_size=self.batch_size
                    )

                if self._enable_profiling:
                    profile_times['inference'] = (time.perf_counter() - inference_start) * 1000
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
            
            # çµæœã‚’å–å¾—
            if outputs and len(outputs) > 0:
                if isinstance(outputs[0], list) and len(outputs[0]) > 0:
                    result = outputs[0][0]
                else:
                    result = outputs[0]
                
                if isinstance(result, dict):
                    text = result.get('text', '').strip()
                    
                    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
                    confidence = 1.0
                    if 'segments' in result and isinstance(result['segments'], list) and len(result['segments']) > 0:
                        total_logprob = 0
                        segment_count = 0
                        for segment in result['segments']:
                            if isinstance(segment, dict) and 'avg_logprob' in segment:
                                total_logprob += segment['avg_logprob']
                                segment_count += 1
                        
                        if segment_count > 0:
                            avg_logprob = total_logprob / segment_count
                            confidence = np.exp(avg_logprob)
                elif isinstance(result, str):
                    text = result.strip()
                    confidence = 1.0
                else:
                    text = str(result) if result else ""
                    confidence = 1.0
                
                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœã‚’å‡ºåŠ›
                if self._enable_profiling:
                    self._log_profiling_results(profile_times, total_start, audio_data)

                return text, confidence
            else:
                return "", 1.0
                
        except RuntimeError as e:
            if "cuDNN" in str(e) and self.device == 'cuda':
                logger.warning(f"cuDNN error, retrying with CPU: {e}")

                cpu_cache_key = f"whispers2t_{self.model_size}_cpu_float32"
                cpu_model = ModelMemoryCache.get(cpu_cache_key)

                if cpu_model is None:
                    import whisper_s2t
                    cpu_model = whisper_s2t.load_model(
                        model_identifier=self.model_size,
                        backend='CTranslate2',
                        device='cpu',
                        compute_type='float32'
                    )
                    ModelMemoryCache.set(cpu_cache_key, cpu_model, strong=True)

                original_model, original_device = self.model, self.device
                self.model, self.device = cpu_model, 'cpu'

                try:
                    result = self.transcribe(audio_data, sample_rate)
                finally:
                    self.model, self.device = original_model, original_device

                return result
            else:
                logger.error(f"Error during transcription: {e}")
                raise
                
        except Exception as e:
            logger.error(f"Error during transcription: {e}")
            raise
            
    def _log_profiling_results(self, profile_times: Dict, start_time: float, audio_data: np.ndarray) -> None:
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœã‚’ãƒ­ã‚°å‡ºåŠ›"""
        total_time = (time.perf_counter() - start_time) * 1000
        profile_times['total'] = total_time
        audio_duration = len(audio_data) / 16000

        logger.info("=== WhisperS2T Performance Profile ===")
        for key, ms in profile_times.items():
            if key != 'total':
                percentage = (ms / total_time) * 100 if total_time > 0 else 0
                logger.info(f"  {key:12s}: {ms:6.1f}ms ({percentage:4.1f}%)")
        logger.info(f"  {'='*30}")
        logger.info(f"  {'Total':12s}: {total_time:6.1f}ms")
        logger.info(f"  Audio duration: {audio_duration:.2f}s")
        logger.info(f"  Real-time factor: {total_time / 1000 / audio_duration:.2f}x")

    def get_engine_name(self) -> str:
        """ã‚¨ãƒ³ã‚¸ãƒ³åã‚’å–å¾—"""
        size_map = {
            'base': 'Base',
            'large-v3': 'Large-v3'
        }
        return f"WhisperS2T {size_map.get(self.model_size, self.model_size.title())}"
        
    def get_supported_languages(self) -> list:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹è¨€èªã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # WhisperS2Tã¯å¤šè¨€èªå¯¾å¿œ
        return ["ja", "en", "zh", "ko", "es", "fr", "de", "ru", "ar", "pt", "it", "hi"]
        
    def get_required_sample_rate(self) -> int:
        """ã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦æ±‚ã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—"""
        return 16000
        
    def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.model is not None:
            del self.model
            self.model = None

            if self.device == "cuda":
                try:
                    import torch
                    torch.cuda.empty_cache()
                except ImportError:
                    pass
        self._initialized = False

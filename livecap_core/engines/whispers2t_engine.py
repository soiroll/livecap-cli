"""WhisperS2Tã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£… (Template Methodç‰ˆ)"""
import os
import logging
import tempfile
import time
import soundfile as sf
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
import numpy as np

from .base_engine import BaseEngine
from .model_memory_cache import ModelMemoryCache
from .library_preloader import LibraryPreloader
from .whisper_languages import WHISPER_LANGUAGES, WHISPER_LANGUAGES_SET
from livecap_core.languages import Languages

# ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¹è§£æ±ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¨ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from livecap_core.utils import detect_device, get_temp_dir

logger = logging.getLogger(__name__)

# ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆWhisperS2Tã®_MODELSã«ãªã„ãƒ¢ãƒ‡ãƒ«ã¯HuggingFaceãƒ‘ã‚¹ã§æŒ‡å®šï¼‰
MODEL_MAPPING = {
    "tiny": "tiny",
    "base": "base",
    "small": "small",
    "medium": "medium",
    "large-v1": "large-v1",
    "large-v2": "large-v2",
    "large-v3": "large-v3",
    "large-v3-turbo": "deepdml/faster-whisper-large-v3-turbo-ct2",
    "distil-large-v3": "Systran/faster-distil-whisper-large-v3",
}

VALID_MODEL_SIZES = frozenset(MODEL_MAPPING.keys())
VALID_COMPUTE_TYPES = frozenset({"auto", "int8", "int8_float16", "float16", "float32"})

# 128ãƒ¡ãƒ«ãƒãƒ³ã‚¯ãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ï¼ˆv3ãƒ™ãƒ¼ã‚¹ï¼‰
MODELS_REQUIRING_128_MELS = frozenset({"large-v3", "large-v3-turbo", "distil-large-v3"})

# CPUé€Ÿåº¦æ¨å®šå€¤
CPU_SPEED_ESTIMATES = {
    'base': '3-5x real-time',
    'large-v3': '0.1-0.3x real-time (VERY SLOW)',
    'large-v3-turbo': '~0.5x real-time',
}


class WhisperS2TEngine(BaseEngine):
    """WhisperS2TéŸ³å£°èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ (Template Methodç‰ˆ)"""

    def __init__(
        self,
        device: Optional[str] = None,
        # ã‚«ãƒ†ã‚´ãƒªA: ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEngineMetadata.default_params ã§å®šç¾©ï¼‰
        language: str = "ja",
        model_size: str = "large-v3",  # benchmarkäº’æ›æ€§ç¶­æŒï¼ˆæ—§whispers2t_large_v3ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        compute_type: str = "auto",
        batch_size: int = 24,
        use_vad: bool = True,
        **kwargs,
    ):
        """ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–

        Args:
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcpu/cuda/autoï¼‰
            language: è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆISO 639-1ã®2æ–‡å­—ã‚³ãƒ¼ãƒ‰ã€ã¾ãŸã¯åœ°åŸŸã‚³ãƒ¼ãƒ‰ï¼‰
            model_size: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆtiny/base/small/medium/large-v1/large-v2/large-v3/large-v3-turbo/distil-large-v3ï¼‰
            compute_type: é‡å­åŒ–ã‚¿ã‚¤ãƒ—ï¼ˆauto/int8/int8_float16/float16/float32ï¼‰
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            use_vad: VADã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if model_size not in VALID_MODEL_SIZES:
            raise ValueError(
                f"Unsupported model_size: {model_size}. "
                f"Valid options: {', '.join(sorted(VALID_MODEL_SIZES))}"
            )
        if compute_type not in VALID_COMPUTE_TYPES:
            raise ValueError(
                f"Unsupported compute_type: {compute_type}. "
                f"Valid options: {', '.join(sorted(VALID_COMPUTE_TYPES))}"
            )

        # è¨€èªã‚³ãƒ¼ãƒ‰ã®å¤‰æ›ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        # 1. UIè¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆzh-CNç­‰ï¼‰â†’ ASRè¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆzhç­‰ï¼‰ã¸ã®å¤‰æ›
        lang_info = Languages.get_info(language)
        asr_language = lang_info.asr_code if lang_info else language

        # 2. WHISPER_LANGUAGES_SET ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆO(1) lookupï¼‰
        if asr_language not in WHISPER_LANGUAGES_SET:
            raise ValueError(
                f"Unsupported language: {language}. "
                f"WhisperS2T supports 99 languages. See: https://github.com/openai/whisper"
            )

        # engine_name ã‚’çµ±ä¸€ï¼ˆæ—§: f'whispers2t_{model_size}'ï¼‰
        self.engine_name = "whispers2t"
        self.model_size = model_size
        self.language = language  # å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿æŒï¼ˆãƒ­ã‚°/ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        self._asr_language = asr_language  # å¤‰æ›å¾Œã®ã‚³ãƒ¼ãƒ‰ï¼ˆtranscribe()ã§ä½¿ç”¨ï¼‰
        self.batch_size = batch_size
        self.use_vad = use_vad

        # cuDNNè¨­å®šï¼ˆGPUä½¿ç”¨æ™‚ã®å®‰å®šæ€§å‘ä¸Šï¼‰
        os.environ['CUDNN_DETERMINISTIC'] = '1'
        os.environ['CUDNN_BENCHMARK'] = '0'

        # ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•æ¤œå‡ºã¨è¨­å®šï¼ˆå…±é€šé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
        # detect_device() ã¯ Tuple[str, str] ã‚’è¿”ã™ãŸã‚ã€æœ€åˆã®è¦ç´ ã®ã¿ä½¿ç”¨
        # æ³¨: #166 å®Œäº†å¾Œã¯æˆ»ã‚Šå€¤ãŒ str ã«ãªã‚‹
        device_result = detect_device(device, "WhisperS2T")
        self.device = device_result[0] if isinstance(device_result, tuple) else device_result

        # compute_type ã®è§£æ±ºï¼ˆautoã®å ´åˆã¯ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰
        self.compute_type = self._resolve_compute_type(compute_type)

        # å¤§å‹ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ã®è­¦å‘Š
        if self.model_size in ('large-v3', 'large-v3-turbo', 'distil-large-v3') and self.device == 'cpu':
            speed_estimate = CPU_SPEED_ESTIMATES.get(self.model_size, 'SLOW')
            logger.warning(f"âš ï¸ WhisperS2T {self.model_size} on CPU will be {speed_estimate}! Consider using GPU or smaller model.")

        # BaseEngineåˆæœŸåŒ–ï¼ˆget_model_metadata()ãŒå‘¼ã°ã‚Œã‚‹ï¼‰
        # detect_deviceã§å–å¾—ã—ãŸæ­£ã—ã„deviceå€¤ã‚’æ¸¡ã™ï¼ˆNoneã§ã¯ãªãï¼‰
        super().__init__(self.device, **kwargs)

        # äº‹å‰ãƒ­ãƒ¼ãƒ‰é–‹å§‹
        LibraryPreloader.start_preloading('whispers2t')

        # å›ºå®šã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        self._tmp_dir = get_temp_dir("whispers2t")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°è¨­å®šï¼ˆkwargs ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ Falseï¼‰
        self._enable_profiling = kwargs.get('profile', False)

        # åˆæœŸåŒ–å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if self.device == 'cuda':
            logger.info(f"âœ… WhisperS2T {model_size} engine initialized (GPU mode: {self.compute_type})")
        else:
            logger.info(f"WhisperS2T {model_size} engine initialized (CPU mode: {self.compute_type})")

    def _resolve_compute_type(self, compute_type: str) -> str:
        """compute_typeã‚’è§£æ±ºï¼ˆautoã®å ´åˆã¯ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰"""
        if compute_type != "auto":
            return compute_type  # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã‚’å°Šé‡
        # auto: CPUâ†’int8ï¼ˆ1.5å€é«˜é€Ÿï¼‰ã€GPUâ†’float16
        return "int8" if self.device == "cpu" else "float16"

    def _get_n_mels(self) -> int:
        """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸ n_mels å€¤ã‚’å–å¾—"""
        return 128 if self.model_size in MODELS_REQUIRING_128_MELS else 80

    def _get_model_identifier(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’ WhisperS2T ç”¨ã®è­˜åˆ¥å­ã«å¤‰æ›"""
        return MODEL_MAPPING.get(self.model_size, self.model_size)
    
    def get_model_metadata(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        descriptions = {
            'tiny': 'Whisper Tiny - Fastest, lowest accuracy',
            'base': 'Whisper Base - Good balance',
            'small': 'Whisper Small - Better accuracy',
            'medium': 'Whisper Medium - High accuracy',
            'large-v1': 'Whisper Large v1 - Original large model',
            'large-v2': 'Whisper Large v2 - Improved large model',
            'large-v3': 'Whisper Large v3 - Best accuracy',
            'large-v3-turbo': 'Whisper Large v3 Turbo - 8x faster than v3',
            'distil-large-v3': 'Distil Whisper Large v3 - 6x faster, ~1% WER increase',
        }

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ¤å®š
        if 'v3' in self.model_size:
            version = 'v3'
        elif 'v2' in self.model_size:
            version = 'v2'
        elif 'v1' in self.model_size:
            version = 'v1'
        else:
            version = 'v2'  # tiny/base/small/medium ã¯v2ç›¸å½“

        return {
            'name': f'openai/whisper-{self.model_size}',
            'version': version,
            'format': 'ct2',
            'language': 'multilingual',
            'description': descriptions.get(self.model_size, f'Whisper {self.model_size}'),
            'model_size': self.model_size,
            'n_mels': self._get_n_mels(),
        }
    
    def _check_dependencies(self) -> None:
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ (Step 1: 0-10%)"""
        self.report_progress(5, self.get_status_message("checking_availability", engine_name="WhisperS2T"))
        LibraryPreloader.wait_for_preload(timeout=2.0)

        try:
            import whisper_s2t
        except ImportError:
            raise ImportError("WhisperS2T is not installed. Please run: pip install whisper-s2t")

        if self.device == 'cuda':
            try:
                import torch
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = False
                torch.backends.cudnn.deterministic = True
            except ImportError:
                pass

        self.report_progress(10, self.get_status_message("dependencies_complete"))
    
    def _get_local_model_path(self, models_dir: Path) -> Path:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾— (Step 2: 10-15%)"""
        model_path = models_dir / f"whisper-{self.model_size}"
        self.report_progress(15, self.get_status_message("model_info", model_name=f"whisper-{self.model_size}"))
        return model_path

    def _is_model_cached(self, model_path: Path) -> bool:
        """WhisperS2Tã¯å†…éƒ¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ç®¡ç†ã™ã‚‹ãŸã‚ã€å¸¸ã«Trueã‚’è¿”ã™"""
        return True

    def _verify_model_integrity(self, model_path) -> bool:
        """WhisperS2Tã¯ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿä½“ã«ä¾å­˜ã—ãªã„ãŸã‚å¸¸ã«True"""
        return True
    
    def _download_model(self, target_path: Path, progress_callback, model_manager=None) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Step 3: 15-70%)"""
        self.report_progress(70, self.get_status_message("model_ready", engine_name="WhisperS2T", model_name=self.model_size))
    
    def _load_model_from_path(self, model_path: Path) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ (Step 4: 70-90%)"""
        import whisper_s2t

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        cache_key = f"whispers2t_{self.model_size}_{self.device}_{self.compute_type}"
        cached_model = ModelMemoryCache.get(cache_key)

        if cached_model is not None:
            logger.info(f"ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—: {cache_key}")
            self.report_progress(90, self.get_status_message("loading_from_memory_cache"))
            return cached_model

        # å¤§å‹ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªè­¦å‘Š
        if self.model_size in ('large-v3', 'large-v3-turbo', 'distil-large-v3') and self.device == 'cpu':
            logger.warning("ğŸ“Š WhisperS2T large model requires ~10GB system memory on CPU")

        self.report_progress(75, self.get_status_message("initializing_model", engine_name="WhisperS2T", model_name=self.model_size))

        # ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ã‚’å–å¾—ï¼ˆHuggingFaceãƒ‘ã‚¹ã¸ã®å¤‰æ›ï¼‰
        model_identifier = self._get_model_identifier()
        n_mels = self._get_n_mels()

        try:
            # WhisperS2Tãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆn_mels ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
            model = whisper_s2t.load_model(
                model_identifier=model_identifier,
                backend='CTranslate2',
                device=self.device,
                compute_type=self.compute_type,
                n_mels=n_mels,  # v3ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ã¯128ã‚’æŒ‡å®š
            )

            self.report_progress(85, self.get_status_message("initialization_success", engine_name="WhisperS2T"))

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            ModelMemoryCache.set(cache_key, model, strong=True)

            if self.device == 'cuda':
                logger.info(f"âœ… WhisperS2T {self.model_size} loaded on GPU (n_mels={n_mels})")
            else:
                speed_estimate = CPU_SPEED_ESTIMATES.get(self.model_size, '')
                if speed_estimate:
                    logger.info(f"ğŸ“Š WhisperS2T {self.model_size} on CPU: {speed_estimate}")

            self.report_progress(90, self.get_status_message("model_ready_simple", engine_name="WhisperS2T"))
            return model

        except Exception as e:
            if "cuDNN" in str(e) and self.device == 'cuda':
                logger.warning(f"cuDNN error detected, falling back to CPU: {e}")
                self.device = 'cpu'
                self.compute_type = 'int8'  # CPU fallback ã§ã‚‚ int8 ã‚’ä½¿ç”¨

                model = whisper_s2t.load_model(
                    model_identifier=model_identifier,
                    backend='CTranslate2',
                    device='cpu',
                    compute_type='int8',
                    n_mels=n_mels,
                )

                ModelMemoryCache.set(f"whispers2t_{self.model_size}_cpu_int8", model, strong=True)
                self.report_progress(90, self.get_status_message("model_ready_cpu_mode", engine_name="WhisperS2T"))
                return model
            else:
                logger.error(f"Failed to load WhisperS2T model: {e}")
                raise
    
    def _configure_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«è¨­å®š (Step 5: 90-100%)"""
        if self.model is None:
            raise RuntimeError("Model not loaded")

        self.report_progress(95, self.get_status_message("final_configuration", engine_name="WhisperS2T"))

        logger.info(f"WhisperS2T {self.model_size} initialized")

        self.report_progress(100, self.get_status_message("initialization_complete", engine_name="WhisperS2T"))
    
    def transcribe(self, audio_data: np.ndarray, sample_rate: int) -> Tuple[str, float]:
        """
        éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹

        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpyé…åˆ—ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

        Returns:
            (transcription_text, confidence_score)ã®ã‚¿ãƒ—ãƒ«
        """
        # WhisperS2Tã¯é•·æ™‚é–“éŸ³å£°ã‚‚å‡¦ç†å¯èƒ½
        # ç’°å¢ƒå¤‰æ•°åˆ‡æ›¿ã¯ä¸è¦ï¼ˆå›ºå®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼‰
        return self._transcribe_single_chunk(audio_data, sample_rate)
    
    def _transcribe_single_chunk(self, audio_data: np.ndarray, sample_rate: int) -> Tuple[str, float]:
        """
        å˜ä¸€ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰

        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpyé…åˆ—ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

        Returns:
            (transcription_text, confidence_score)ã®ã‚¿ãƒ—ãƒ«
        """
        if not self._initialized or self.model is None:
            raise RuntimeError("Engine not initialized. Call load_model() first.")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹
        if self._enable_profiling:
            profile_times = {}
            total_start = time.perf_counter()
            
        # 16kHzã«å¤‰æ›
        required_sr = 16000
        if sample_rate != required_sr:
            if self._enable_profiling:
                resample_start = time.perf_counter()

            from scipy.signal import resample_poly
            from math import gcd

            g = gcd(sample_rate, required_sr)
            audio_data = resample_poly(audio_data, required_sr // g, sample_rate // g).astype(np.float32)

            if self._enable_profiling:
                profile_times['resample'] = (time.perf_counter() - resample_start) * 1000
            
        # æ­£è¦åŒ–
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        if np.abs(audio_data).max() > 1.0:
            audio_data = audio_data / np.abs(audio_data).max()
            
        
        # éŸ³å£°ãŒçŸ­ã™ãã‚‹å ´åˆã®å‡¦ç†
        min_samples = int(0.1 * 16000)  # æœ€å°0.1ç§’
        if len(audio_data) < min_samples:
            return "", 1.0
            
        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰ã¯ __init__ ã§å¤‰æ›æ¸ˆã¿ï¼ˆ_asr_language ã‚’ä½¿ç”¨ï¼‰
            whisper_language = self._asr_language

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            if self._enable_profiling:
                io_start = time.perf_counter()

            with tempfile.NamedTemporaryFile(dir=self._tmp_dir, suffix='.wav', delete=False) as tmp_file:
                tmp_path = tmp_file.name
                sf.write(tmp_path, audio_data, 16000)

            if self._enable_profiling:
                profile_times['wav_write'] = (time.perf_counter() - io_start) * 1000

            try:
                if self._enable_profiling:
                    inference_start = time.perf_counter()

                # WhisperS2Tã§æ–‡å­—èµ·ã“ã—
                if self.use_vad:
                    outputs = self.model.transcribe_with_vad(
                        [tmp_path],
                        lang_codes=[whisper_language],
                        tasks=["transcribe"],
                        initial_prompts=[None],
                        batch_size=self.batch_size
                    )
                else:
                    outputs = self.model.transcribe(
                        [tmp_path],
                        lang_codes=[whisper_language],
                        tasks=["transcribe"],
                        initial_prompts=[None],
                        batch_size=self.batch_size
                    )

                if self._enable_profiling:
                    profile_times['inference'] = (time.perf_counter() - inference_start) * 1000
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
            
            # çµæœã‚’å–å¾—
            if outputs and len(outputs) > 0:
                if isinstance(outputs[0], list) and len(outputs[0]) > 0:
                    result = outputs[0][0]
                else:
                    result = outputs[0]
                
                if isinstance(result, dict):
                    text = result.get('text', '').strip()
                    
                    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
                    confidence = 1.0
                    if 'segments' in result and isinstance(result['segments'], list) and len(result['segments']) > 0:
                        total_logprob = 0
                        segment_count = 0
                        for segment in result['segments']:
                            if isinstance(segment, dict) and 'avg_logprob' in segment:
                                total_logprob += segment['avg_logprob']
                                segment_count += 1
                        
                        if segment_count > 0:
                            avg_logprob = total_logprob / segment_count
                            confidence = np.exp(avg_logprob)
                elif isinstance(result, str):
                    text = result.strip()
                    confidence = 1.0
                else:
                    text = str(result) if result else ""
                    confidence = 1.0
                
                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœã‚’å‡ºåŠ›
                if self._enable_profiling:
                    self._log_profiling_results(profile_times, total_start, audio_data)

                return text, confidence
            else:
                return "", 1.0
                
        except RuntimeError as e:
            if "cuDNN" in str(e) and self.device == 'cuda':
                logger.warning(f"cuDNN error, retrying with CPU: {e}")

                cpu_cache_key = f"whispers2t_{self.model_size}_cpu_float32"
                cpu_model = ModelMemoryCache.get(cpu_cache_key)

                if cpu_model is None:
                    import whisper_s2t
                    cpu_model = whisper_s2t.load_model(
                        model_identifier=self.model_size,
                        backend='CTranslate2',
                        device='cpu',
                        compute_type='float32'
                    )
                    ModelMemoryCache.set(cpu_cache_key, cpu_model, strong=True)

                original_model, original_device = self.model, self.device
                self.model, self.device = cpu_model, 'cpu'

                try:
                    result = self.transcribe(audio_data, sample_rate)
                finally:
                    self.model, self.device = original_model, original_device

                return result
            else:
                logger.error(f"Error during transcription: {e}")
                raise
                
        except Exception as e:
            logger.error(f"Error during transcription: {e}")
            raise
            
    def _log_profiling_results(self, profile_times: Dict, start_time: float, audio_data: np.ndarray) -> None:
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœã‚’ãƒ­ã‚°å‡ºåŠ›"""
        total_time = (time.perf_counter() - start_time) * 1000
        profile_times['total'] = total_time
        audio_duration = len(audio_data) / 16000

        logger.info("=== WhisperS2T Performance Profile ===")
        for key, ms in profile_times.items():
            if key != 'total':
                percentage = (ms / total_time) * 100 if total_time > 0 else 0
                logger.info(f"  {key:12s}: {ms:6.1f}ms ({percentage:4.1f}%)")
        logger.info(f"  {'='*30}")
        logger.info(f"  {'Total':12s}: {total_time:6.1f}ms")
        logger.info(f"  Audio duration: {audio_duration:.2f}s")
        logger.info(f"  Real-time factor: {total_time / 1000 / audio_duration:.2f}x")

    def get_engine_name(self) -> str:
        """ã‚¨ãƒ³ã‚¸ãƒ³åã‚’å–å¾—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è¡¨ç¤ºç”¨ï¼‰"""
        size_map = {
            'tiny': 'Tiny',
            'base': 'Base',
            'small': 'Small',
            'medium': 'Medium',
            'large-v1': 'Large-v1',
            'large-v2': 'Large-v2',
            'large-v3': 'Large-v3',
            'large-v3-turbo': 'Large-v3 Turbo',
            'distil-large-v3': 'Distil Large-v3',
        }
        return f"WhisperS2T {size_map.get(self.model_size, self.model_size.title())}"

    def get_supported_languages(self) -> list:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹è¨€èªã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # WhisperS2Tã¯99è¨€èªå¯¾å¿œ
        return list(WHISPER_LANGUAGES)
        
    def get_required_sample_rate(self) -> int:
        """ã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦æ±‚ã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—"""
        return 16000
        
    def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.model is not None:
            del self.model
            self.model = None

            if self.device == "cuda":
                try:
                    import torch
                    torch.cuda.empty_cache()
                except ImportError:
                    pass
        self._initialized = False
